{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Load Data\n",
    "\n",
    "Start by importing the simulator data from the training_data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def read_csv(filepath, num_features=7, delimiter=';'):\n",
    "    data_array = np.array(np.zeros(shape=num_features), ndmin=2)    \n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        annotations_reader = csv.reader(csvfile, delimiter=delimiter, quotechar='|')\n",
    "        for row in annotations_reader:\n",
    "            data_array = np.vstack((data_array, np.array(row, ndmin=2)))\n",
    "    return data_array[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_image_paths shape (255,)\n",
      "left_image_paths shape (255,)\n",
      "right_image_paths shape (255,)\n",
      "steering_angles shape (255,)\n"
     ]
    }
   ],
   "source": [
    "DRIVING_LOG_CSV = 'driving_log.csv'\n",
    "annotations = read_csv(os.path.join('merged_data', DRIVING_LOG_CSV), delimiter=',')\n",
    "center_image_paths, steering_angles = annotations[:,0], annotations[:,3]\n",
    "left_image_paths, right_image_paths = annotations[:,1], annotations[:,2]\n",
    "\n",
    "print('center_image_paths shape', center_image_paths.shape)\n",
    "print('left_image_paths shape', left_image_paths.shape)\n",
    "print('right_image_paths shape', right_image_paths.shape)\n",
    "print('steering_angles shape', steering_angles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "TRAINING_DATA_DIR = 'merged_data'\n",
    "def load_images(image_paths):\n",
    "    images = np.array(np.zeros(shape=(1, 160, 320, 3)), ndmin=2)\n",
    "    for path in image_paths:\n",
    "        image = np.array(cv2.imread(os.path.join(TRAINING_DATA_DIR,path)), ndmin=4)\n",
    "        images = np.vstack((images, image))\n",
    "    return images\n",
    "\n",
    "center_images = load_images(center_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 160, 320, 3)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "\n",
    "\n",
    "center_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('train.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# TODO: Load the feature data to the variable X_train\n",
    "\n",
    "X_train, y_train = data['features'], data['labels']\n",
    "\n",
    "# TODO: Load the label data to the variable y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert np.array_equal(X_train, data['features']), 'X_train not set to data[\\'features\\'].'\n",
    "# assert np.array_equal(y_train, data['labels']), 'y_train not set to data[\\'labels\\'].'\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "\n",
    "1. Shuffle the data\n",
    "2. Normalize the features using Min-Max scaling between -0.5 and 0.5\n",
    "3. One-Hot Encode the labels\n",
    "\n",
    "### Shuffle the data\n",
    "Hint: You can use the [scikit-learn shuffle](http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html) function to shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Shuffle the data\n",
    "perm = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(perm)\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert X_train.shape == data['features'].shape, 'X_train has changed shape. The shape shouldn\\'t change when shuffling.'\n",
    "# assert y_train.shape == data['labels'].shape, 'y_train has changed shape. The shape shouldn\\'t change when shuffling.'\n",
    "# assert not np.array_equal(X_train, data['features']), 'X_train not shuffled.'\n",
    "# assert not np.array_equal(y_train, data['labels']), 'y_train not shuffled.'\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the features\n",
    "Hint: You solved this in [TensorFlow lab](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/lab.ipynb) Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_normalized shape (39209, 32, 32, 3)\n",
      "X_test shape (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Normalize the data features to the variable X_normalized\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [-0.5,0.5]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    a=-0.5\n",
    "    b=0.5\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + (((image_data - grayscale_min)*(b-a))/(grayscale_max - grayscale_min))\n",
    "\n",
    "X_normalized = normalize_grayscale(X_train)\n",
    "print('X_normalized shape', X_normalized.shape)\n",
    "print('X_test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert math.isclose(np.min(X_normalized), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_normalized), 0.5, abs_tol=1e-5), 'The range of the training data is: {} to {}.  It must be -0.5 to 0.5'.format(np.min(X_normalized), np.max(X_normalized))\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "\n",
    "1. Compile the network using adam optimizer and categorical_crossentropy loss function.\n",
    "2. Train the network for ten epochs and validate with 20% of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1 (Convolution2D)           (None, 15, 15, 96)    2688        input[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "maxpool_1 (MaxPooling2D)         (None, 7, 7, 96)      0           conv_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "fire2_squeeze (Convolution2D)    (None, 7, 7, 16)      1552        maxpool_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fire2_expand1 (Convolution2D)    (None, 7, 7, 64)      1088        fire2_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire2_expand2 (Convolution2D)    (None, 7, 7, 64)      9280        fire2_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "merge_inputs_for_fire_2 (Merge)  (None, 14, 7, 64)     0           fire2_expand1[0][0]              \n",
      "                                                                   fire2_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire_2 (Activation)              (None, 14, 7, 64)     0           merge_inputs_for_fire_2[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "fire3_squeeze (Convolution2D)    (None, 14, 7, 16)     1040        fire_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "fire3_expand1 (Convolution2D)    (None, 14, 7, 64)     1088        fire3_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire3_expand2 (Convolution2D)    (None, 14, 7, 64)     9280        fire3_squeeze[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "merge_inputs_for_fire_3 (Merge)  (None, 28, 7, 64)     0           fire3_expand1[0][0]              \n",
      "                                                                   fire3_expand2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "fire_3 (Activation)              (None, 28, 7, 64)     0           merge_inputs_for_fire_3[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "fire9_dropout (Dropout)          (None, 28, 7, 64)     0           fire_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_10 (Convolution2D)          (None, 28, 7, 43)     2795        fire9_dropout[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "avgpool_10 (GlobalAveragePooling2(None, 28)            0           conv_10[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 28)            0           avgpool_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "fc_final (Dense)                 (None, 43)            1247        flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Activation)              (None, 43)            0           fc_final[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 30058\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Not using data augmentation...\n",
      "Train on 30504 samples, validate on 8705 samples\n",
      "Epoch 1/20\n",
      "30504/30504 [==============================] - 87s - loss: 3.7449 - acc: 0.0550 - val_loss: 3.7245 - val_acc: 0.0581\n",
      "Epoch 2/20\n",
      " 9984/30504 [========>.....................] - ETA: 52s - loss: 3.7227 - acc: 0.0666"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-520fadeb275e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m model.fit({'input':X_normalized,'output':Y_train}, batch_size=batch_size,\n\u001b[0;32m--> 121\u001b[0;31m          nb_epoch=nb_epoch, validation_split=0.222, shuffle=True,callbacks=[callback1])\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# model.fit({'input':X_normalized,'output':Y_train}, batch_size=batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/keras/legacy/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m                                       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                                       \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                                       sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     def evaluate(self, data, batch_size=128,\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Graph, Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "import cv2\n",
    "import sys\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 43\n",
    "nb_epoch = 50\n",
    "#data_augmentation = True\n",
    "\n",
    "# Using SqueezeNet\n",
    "model = Graph()\n",
    "model.add_input(name='input',input_shape=(32,32,3))\n",
    "\n",
    "#conv_1: \n",
    "model.add_node(Convolution2D(96, 3, 3, activation='relu', init='glorot_uniform',\n",
    "                             subsample=(2,2),border_mode='valid'),name='conv_1', input='input')\n",
    "\n",
    "#maxpool_1\n",
    "model.add_node(MaxPooling2D(pool_size=(3,3),strides=(2,2)),name='maxpool_1', input='conv_1')\n",
    "\n",
    "#fire module 1\n",
    "model.add_node(Convolution2D(16, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire2_squeeze', input='maxpool_1')\n",
    "model.add_node(Convolution2D(64, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire2_expand1', input='fire2_squeeze')\n",
    "model.add_node(Convolution2D(64, 3, 3, activation='relu', init='glorot_uniform',border_mode='same'),name='fire2_expand2', input='fire2_squeeze')\n",
    "model.add_node(Activation(\"linear\"),name='fire_2', inputs=[\"fire2_expand1\",\"fire2_expand2\"], merge_mode=\"concat\", concat_axis=1)\n",
    "\n",
    "#fire module 2\n",
    "model.add_node(Convolution2D(16, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire3_squeeze', input='fire_2')\n",
    "model.add_node(Convolution2D(64, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire3_expand1', input='fire3_squeeze')\n",
    "model.add_node(Convolution2D(64, 3, 3, activation='relu', init='glorot_uniform',border_mode='same'),name='fire3_expand2', input='fire3_squeeze')\n",
    "model.add_node(Activation(\"linear\"),name='fire_3', inputs=[\"fire3_expand1\",\"fire3_expand2\"], merge_mode=\"concat\", concat_axis=1)\n",
    "\n",
    "# #fire module  3\n",
    "# model.add_node(Convolution2D(32, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire4_squeeze', input='fire_3')\n",
    "# model.add_node(Convolution2D(128, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire4_expand1', input='fire4_squeeze')\n",
    "# model.add_node(Convolution2D(128, 3, 3, activation='relu', init='glorot_uniform',border_mode='same'),name='fire4_expand2', input='fire4_squeeze')\n",
    "# model.add_node(Activation(\"linear\"),name='fire_4', inputs=[\"fire4_expand1\",\"fire4_expand2\"], merge_mode=\"concat\", concat_axis=1)\n",
    "\n",
    "# #maxpool 4\n",
    "# model.add_node(MaxPooling2D((2,2)),name='maxpool_4', input='fire_4')\n",
    "\n",
    "# #fire module  5\n",
    "# model.add_node(Convolution2D(32, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire5_squeeze', input='maxpool_4')\n",
    "# model.add_node(Convolution2D(128, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire5_expand1', input='fire5_squeeze')\n",
    "# model.add_node(Convolution2D(128, 3, 3, activation='relu', init='glorot_uniform',border_mode='same'),name='fire5_expand2', input='fire5_squeeze')\n",
    "# model.add_node(Activation(\"linear\"),name='fire_5', inputs=[\"fire5_expand1\",\"fire5_expand2\"], merge_mode=\"concat\", concat_axis=1)\n",
    "\n",
    "# #fire module 6\n",
    "# model.add_node(Convolution2D(48, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire6_squeeze', input='fire_5')\n",
    "# model.add_node(Convolution2D(192, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire6_expand1', input='fire6_squeeze')\n",
    "# model.add_node(Convolution2D(192, 3, 3, activation='relu', init='glorot_uniform',border_mode='same'),name='fire6_expand2', input='fire6_squeeze')\n",
    "# model.add_node(Activation(\"linear\"),name='fire_6', inputs=[\"fire6_expand1\",\"fire6_expand2\"], merge_mode=\"concat\", concat_axis=1)\n",
    "\n",
    "# #fire module 7\n",
    "# model.add_node(Convolution2D(48, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire7_squeeze', input='fire_6')\n",
    "# model.add_node(Convolution2D(192, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire7_expand1', input='fire7_squeeze')\n",
    "# model.add_node(Convolution2D(192, 3, 3, activation='relu', init='glorot_uniform',border_mode='same'),name='fire7_expand2', input='fire7_squeeze')\n",
    "# model.add_node(Activation(\"linear\"),name='fire_7', inputs=[\"fire7_expand1\",\"fire7_expand2\"], merge_mode=\"concat\", concat_axis=1)\n",
    "\n",
    "# #fire module 8\n",
    "# model.add_node(Convolution2D(64, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire8_squeeze', input='fire_7')\n",
    "# model.add_node(Convolution2D(256, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire8_expand1', input='fire8_squeeze')\n",
    "# model.add_node(Convolution2D(256, 3, 3, activation='relu', init='glorot_uniform',border_mode='same'),name='fire8_expand2', input='fire8_squeeze')\n",
    "# model.add_node(Activation(\"linear\"),name='fire_8', inputs=[\"fire8_expand1\",\"fire8_expand2\"], merge_mode=\"concat\", concat_axis=1)\n",
    "\n",
    "# #maxpool 8\n",
    "# model.add_node(MaxPooling2D((2,2)),name='maxpool_8', input='fire_8')\n",
    "\n",
    "# #fire module 9\n",
    "# model.add_node(Convolution2D(64, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire9_squeeze', input='maxpool_8')\n",
    "# model.add_node(Convolution2D(256, 1, 1, activation='relu', init='glorot_uniform',border_mode='same'),name='fire9_expand1', input='fire9_squeeze')\n",
    "# model.add_node(Convolution2D(256, 3, 3, activation='relu', init='glorot_uniform',border_mode='same'),name='fire9_expand2', input='fire9_squeeze')\n",
    "# model.add_node(Activation(\"linear\"),name='fire_9', inputs=[\"fire9_expand1\",\"fire9_expand2\"], merge_mode=\"concat\", concat_axis=1)\n",
    "\n",
    "model.add_node(Dropout(0.5),input='fire_3',name='fire9_dropout')\n",
    "\n",
    "#conv_10\n",
    "model.add_node(Convolution2D(nb_classes, 1, 1, activation='relu', init='glorot_uniform',border_mode='valid'),\n",
    "               name='conv_10', input='fire9_dropout')\n",
    "\n",
    "#avgpool_10 - Global Average Pooling\n",
    "model.add_node(GlobalAveragePooling2D((13,13)),name='avgpool_10', input='conv_10')\n",
    "\n",
    "model.add_node(Flatten(),name='flatten',input='avgpool_10')\n",
    "model.add_node(Dense(nb_classes, activation='softmax'),input='flatten', name='fc_final')\n",
    "\n",
    "model.add_node(Activation(\"softmax\"),input='fc_final',name='softmax')\n",
    "model.add_output(name='output',input='softmax')\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "#model.compile(optimizer=adam, loss={'output':'categorical_crossentropy'})\n",
    "\n",
    "model.compile(loss={'output':'categorical_crossentropy'},\n",
    "             optimizer=adam,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('Not using data augmentation...')\n",
    "\n",
    "callback1 = ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto')\n",
    "\n",
    "model.fit({'input':X_normalized,'output':Y_train}, batch_size=batch_size,\n",
    "         nb_epoch=nb_epoch, validation_split=0.222, shuffle=True,callbacks=[callback1])\n",
    "\n",
    "# model.fit({'input':X_normalized,'output':Y_train}, batch_size=batch_size,\n",
    "#           nb_epoch=nb_epoch, validation_data={'input':X_test,'output':Y_test}, shuffle=True,callbacks=[callback1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Multi-Layer Feedforward Network\n",
    "\n",
    "Build a multi-layer feedforward neural network to classify the traffic sign images.\n",
    "\n",
    "1. Set the first layer to a `Flatten` layer with the `input_shape` set to (32, 32, 3)\n",
    "2. Set the second layer to `Dense` layer width to 128 output. \n",
    "3. Use a ReLU activation function after the second layer.\n",
    "4. Set the output layer width to 43, since there are 43 classes in the dataset.\n",
    "5. Use a softmax activation function after the output layer.\n",
    "\n",
    "To get started, review the Keras documentation about [models](https://keras.io/models/sequential/) and [layers](https://keras.io/layers/core/).\n",
    "\n",
    "The Keras example of a [Multi-Layer Perceptron](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py) network is similar to what you need to do here. Use that as a guide, but keep in mind that there are a number of differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode the labels\n",
    "Hint: You can use the [scikit-learn LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) function to one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# import collections\n",
    "\n",
    "# assert y_one_hot.shape == (39209, 43), 'y_one_hot is not the correct shape.  It\\'s {}, it should be (39209, 43)'.format(y_one_hot.shape)\n",
    "# assert next((False for y in y_one_hot if collections.Counter(y) != {0: 42, 1: 1}), True), 'y_one_hot not one-hot encoded.'\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: One Hot encode the labels to the variable y_one_hot\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "\n",
    "y_one_hot = label_binarizer.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "# TODO: Build a Multi-layer feedforward neural network with Keras here.\n",
    "model.add(Flatten(input_shape=(32,32,3)))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "# from keras.layers.core import Dense, Activation, Flatten\n",
    "# from keras.activations import relu, softmax\n",
    "\n",
    "# def check_layers(layers, true_layers):\n",
    "#     assert len(true_layers) != 0, 'No layers found'\n",
    "#     for layer_i in range(len(layers)):\n",
    "#         assert isinstance(true_layers[layer_i], layers[layer_i]), 'Layer {} is not a {} layer'.format(layer_i+1, layers[layer_i].__name__)\n",
    "#     assert len(true_layers) == len(layers), '{} layers found, should be {} layers'.format(len(true_layers), len(layers))\n",
    "\n",
    "# check_layers([Flatten, Dense, Activation, Dense, Activation], model.layers)\n",
    "\n",
    "# assert model.layers[0].input_shape == (None, 32, 32, 3), 'First layer input shape is wrong, it should be (32, 32, 3)'\n",
    "# assert model.layers[1].output_shape == (None, 128), 'Second layer output is wrong, it should be (128)'\n",
    "# assert model.layers[2].activation == relu, 'Third layer not a relu activation layer'\n",
    "# assert model.layers[3].output_shape == (None, 43), 'Fourth layer output is wrong, it should be (43)'\n",
    "# assert model.layers[4].activation == softmax, 'Fifth layer not a softmax activation layer'\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "40000/40000 [==============================] - 18s - loss: 1.7018 - acc: 0.4076 - val_loss: 1.5596 - val_acc: 0.4552\n",
      "Epoch 2/6\n",
      "40000/40000 [==============================] - 19s - loss: 1.4755 - acc: 0.4859 - val_loss: 1.5052 - val_acc: 0.4746\n",
      "Epoch 3/6\n",
      "40000/40000 [==============================] - 21s - loss: 1.3888 - acc: 0.5144 - val_loss: 1.4721 - val_acc: 0.4931\n",
      "Epoch 4/6\n",
      "40000/40000 [==============================] - 20s - loss: 1.3176 - acc: 0.5417 - val_loss: 1.4398 - val_acc: 0.5006\n",
      "Epoch 5/6\n",
      "40000/40000 [==============================] - 20s - loss: 1.2611 - acc: 0.5591 - val_loss: 1.4610 - val_acc: 0.4978\n",
      "Epoch 6/6\n",
      "40000/40000 [==============================] - 23s - loss: 1.2142 - acc: 0.5768 - val_loss: 1.5130 - val_acc: 0.4829\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compile and train the model here.\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_normalized, y_one_hot, nb_epoch=6, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# assert model.loss == 'categorical_crossentropy', 'Not using categorical_crossentropy loss function'\n",
    "# assert isinstance(model.optimizer, Adam), 'Not using adam optimizer'\n",
    "# assert len(history.history['acc']) == 10, 'You\\'re using {} epochs when you need to use 10 epochs.'.format(len(history.history['acc']))\n",
    "\n",
    "# assert history.history['acc'][-1] > 0.92, 'The training accuracy was: %.3f. It shoud be greater than 0.92' % history.history['acc'][-1]\n",
    "# assert history.history['val_acc'][-1] > 0.85, 'The validation accuracy is: %.3f. It shoud be greater than 0.85' % history.history['val_acc'][-1]\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "1. Re-construct the previous network\n",
    "2. Add a [convolutional layer](https://keras.io/layers/convolutional/#convolution2d) with 32 filters, a 3x3 kernel, and valid padding before the flatten layer.\n",
    "3. Add a ReLU activation after the convolutional layer.\n",
    "\n",
    "Hint 1: The Keras example of a [convolutional neural network](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) for MNIST would be a good example to review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Re-construct the network and add a convolutional layer before the flatten layer.\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers import Convolution2D\n",
    "\n",
    "#nb_classes = 43\n",
    "\n",
    "nb_classes = 10\n",
    "nb_epoch = 5\n",
    "\n",
    "# Image Processing Constants\n",
    "NB_ROWS, NB_COLS, NB_CHANNELS = (32, 32, 3)\n",
    "IMG_SHAPE = (NB_ROWS, NB_COLS, NB_CHANNELS) if NB_CHANNELS > 1 else (NB_ROWS, NB_COLS)\n",
    "\n",
    "# Convolutional HyperParameters\n",
    "nb_filters = 32\n",
    "kernel_size = (3,3)\n",
    "\n",
    "model = Sequential()\n",
    "#Add Convolution Here\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], \n",
    "                        input_shape=IMG_SHAPE, border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "# from keras.layers.core import Dense, Activation, Flatten\n",
    "# from keras.layers.convolutional import Convolution2D\n",
    "\n",
    "# check_layers([Convolution2D, Activation, Flatten, Dense, Activation, Dense, Activation], model.layers)\n",
    "\n",
    "# assert model.layers[0].input_shape == (None, 32, 32, 3), 'First layer input shape is wrong, it should be (32, 32, 3)'\n",
    "# assert model.layers[0].nb_filter == 32, 'Wrong number of filters, it should be 32'\n",
    "# assert model.layers[0].nb_col == model.layers[0].nb_row == 3, 'Kernel size is wrong, it should be a 3x3'\n",
    "# assert model.layers[0].border_mode == 'valid', 'Wrong padding, it should be valid'\n",
    "\n",
    "# model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "# history = model.fit(X_normalized, y_one_hot, batch_size=128, nb_epoch=2, validation_split=0.2)\n",
    "# assert(history.history['val_acc'][-1] > 0.91), \"The validation accuracy is: %.3f.  It should be greater than 0.91\" % history.history['val_acc'][-1]\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "1. Re-construct the network\n",
    "2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/#maxpooling2d) immediately following your convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Re-construct the network and add a pooling layer after the convolutional layer.\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "nb_classes = 43\n",
    "nb_epoch = 10\n",
    "\n",
    "# Image Processing Constants\n",
    "NB_ROWS, NB_COLS, NB_CHANNELS = (32, 32, 3)\n",
    "IMG_SHAPE = (NB_ROWS, NB_COLS, NB_CHANNELS) if NB_CHANNELS > 1 else (NB_ROWS, NB_COLS)\n",
    "\n",
    "# Convolutional HyperParameters\n",
    "nb_filters = 32\n",
    "kernel_size = (3,3)\n",
    "\n",
    "model = Sequential()\n",
    "#Add Convolution Here\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], \n",
    "                        input_shape=IMG_SHAPE, border_mode='valid'))\n",
    "#Add Pooling Layer Here\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "# from keras.layers.core import Dense, Activation, Flatten\n",
    "# from keras.layers.convolutional import Convolution2D\n",
    "# from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "# check_layers([Convolution2D, MaxPooling2D, Activation, Flatten, Dense, Activation, Dense, Activation], model.layers)\n",
    "# assert model.layers[1].pool_size == (2, 2), 'Second layer must be a max pool layer with pool size of 2x2'\n",
    "\n",
    "# model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "# history = model.fit(X_normalized, y_one_hot, batch_size=128, nb_epoch=2, validation_split=0.2)\n",
    "# assert(history.history['val_acc'][-1] > 0.91), \"The validation accuracy is: %.3f.  It should be greater than 0.91\" % history.history['val_acc'][-1]\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "1. Re-construct the network\n",
    "2. Add a [dropout](https://keras.io/layers/core/#dropout) layer after the pooling layer. Set the dropout rate to 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Re-construct the network and add a convolutional layer before the flatten layer.\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "\n",
    "nb_classes = 43\n",
    "nb_epoch = 10\n",
    "\n",
    "# Image Processing Constants\n",
    "NB_ROWS, NB_COLS, NB_CHANNELS = (32, 32, 3)\n",
    "IMG_SHAPE = (NB_ROWS, NB_COLS, NB_CHANNELS) if NB_CHANNELS > 1 else (NB_ROWS, NB_COLS)\n",
    "\n",
    "# Convolutional HyperParameters\n",
    "nb_filters = 32\n",
    "kernel_size = (3,3)\n",
    "\n",
    "model = Sequential()\n",
    "#Add Convolution Here\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], \n",
    "                        input_shape=IMG_SHAPE, border_mode='valid'))\n",
    "#Add Pooling Layer Here\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "# from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "# from keras.layers.convolutional import Convolution2D\n",
    "# from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "# check_layers([Convolution2D, MaxPooling2D, Dropout, Activation, Flatten, Dense, Activation, Dense, Activation], model.layers)\n",
    "# assert model.layers[2].p == 0.5, 'Third layer should be a Dropout of 50%'\n",
    "\n",
    "# model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "# history = model.fit(X_normalized, y_one_hot, batch_size=128, nb_epoch=2, validation_split=0.2)\n",
    "# assert(history.history['val_acc'][-1] >= 0.91), \"The validation accuracy is: %.3f.  It should be greater than 0.91\" % history.history['val_acc'][-1]\n",
    "# print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Congratulations! You've built a neural network with convolutions, pooling, dropout, and fully-connected layers, all in just a few lines of code.\n",
    "\n",
    "Have fun with the model and see how well you can do! Add more layers, or regularization, or different padding, or batches, or more training epochs.\n",
    "\n",
    "What is the best validation accuracy you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # TODO: Build a model\n",
    "# from keras.layers.core import Dense, Activation, Flatten\n",
    "# from keras.activations import relu, softmax\n",
    "# from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "\n",
    "# #model Constants\n",
    "# nb_epoch = 10\n",
    "# batch_size = 128\n",
    "# nb_classes = 43\n",
    "\n",
    "\n",
    "# # Image Processing Constants\n",
    "# NB_ROWS, NB_COLS, NB_CHANNELS = (32, 32, 3)\n",
    "# IMG_SHAPE = (NB_ROWS, NB_COLS, NB_CHANNELS) if NB_CHANNELS > 1 else (NB_ROWS, NB_COLS)\n",
    "\n",
    "# # Convolutional HyperParameters\n",
    "# nb_filters = 32\n",
    "# kernel_size = (3,3)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], \n",
    "#                         input_shape=IMG_SHAPE, border_mode='valid'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], \n",
    "#                         input_shape=IMG_SHAPE, border_mode='valid'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(nb_classes))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# # TODO: Compile and train the model\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adadelta',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_normalized, y_one_hot, batch_size=batch_size,\n",
    "#          nb_epoch=nb_epoch, validation_split=0.222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Current Validation Accuracy:** (98.09%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Once you've picked out your best model, it's time to test it.\n",
    "\n",
    "Load up the test data and use the [`evaluate()` method](https://keras.io/models/model/#evaluate) to see how well it does.\n",
    "\n",
    "Hint 1: The `evaluate()` method should return an array of numbers. Use the [`metrics_names`](https://keras.io/models/model/) property to get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Load test data\n",
    "with open('test.p', 'rb') as f:\n",
    "    data_test = pickle.load(f)\n",
    "\n",
    "X_test = data_test['features']\n",
    "y_test = data_test['labels']\n",
    "\n",
    "# TODO: Preprocess data & one-hot encode the labels\n",
    "X_normalized_test = normalize_grayscale(X_test)\n",
    "y_one_hot_test = label_binarizer.fit_transform(y_test)\n",
    "\n",
    "# TODO: Evaluate model on test data\n",
    "metrics = model.evaluate(X_normalized_test, y_one_hot_test)\n",
    "for metric_i in range(len(model.metrics_names)):\n",
    "    metric_name = model.metrics_names[metric_i]\n",
    "    metric_value = metrics[metric_i]\n",
    "    print('{}: {}'.format(metric_name, metric_value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Accuracy:** (92.05%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Keras is a great tool to use to quickly build a neural network and evaluate performance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [sdc_dev]",
   "language": "python",
   "name": "Python [sdc_dev]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
